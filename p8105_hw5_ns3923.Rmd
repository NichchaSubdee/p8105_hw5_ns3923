---
title: "p8105_hw5_ns3923"
output: github_document
---

**Set up necessary library**
```{r setup, message=FALSE}
library(tidyverse)
```

## Problem 1

**Goal:** To estimate the probability that at least two people share a birthday for group sizes 2–50 using 10,000 simulations each.

**1) Define a function that simulates random birthdays for a group of size n and returns TRUE if any birthdays are shared.**

```{r P1_1}
set.seed(123)

dup_bday = function(n, days = 365) {
  birthdays = sample.int(days, size = n, replace = TRUE)
  any(duplicated(birthdays))
}

#Check
test_results =
  tibble(
    group_size = c(2, 10, 15, 60),
    shared_bday = map_lgl(group_size, dup_bday)
  )
test_results
```
From the checking table above, smaller group sizes such as 2 and 10 people are less likely to have a shared birthday, as shown by `shared_bday = FALSE`. In contrast, larger groups, especially with 60 people, have a higher chance of at least one shared birthday (`shared_bday = TRUE`). This makes sense because the more people in a group, the greater the chance that two will share the same birthday.

**2) Repeat the simulation 10,000 times for each group size between 2 and 50, and calculate the probability of a shared birthday**
```{r P1_2}
Repeat_group =
  expand_grid(group_size = 2:50, iter = 1:10000) |>
  mutate(dup = map_lgl(group_size, dup_bday)) |>
  group_by(group_size) |>
  summarize(prob_duplicate = mean(dup), .groups = "drop")
```

**3) Make a plot showing the probability as a function of group size**

```{r P1_3}
Repeat_group |>
  ggplot(aes(x = group_size, y = prob_duplicate)) +
  geom_line() +
  geom_point(size = 2, color = "skyblue") +
  theme_minimal() +
  labs(
    title = "Figure 1: Probability of at least 2 people shared birthday",
    x = "Group Size (n)",
    y = "Probability"
  ) 
```

**Explanation:** The trend is upward as the group size increases, meaning that larger groups have a higher probability of two people sharing a birthday. The probability rises and approaches 1 as the group becomes larger. Around 23 people, the chance of a shared birthday is about 50%.

## Problem 2

**Goal:** To conduct a simulation to explore power in a one-sample t-test by simulating 5,000 datasets for each true mean `μ ∈ {0,1,2,3,4,5,6}` with n = 30 and σ = 5.

**1) Set the design elements: n=30, σ=5, μ=0**
```{r P2_1}
set.seed(123)

n        = 30
sigma    = 5
mu_values = 0:6
```

**2) Generate 5,000 datasets from the model x∼Normal[μ,σ]** 
```{r P2_2}
##Create a function to simulate one dataset and run a one-sample t-test
sim_ttest = function(mu) {
  x = rnorm(n, mean = mu, sd = sigma)
  broom::tidy(t.test(x, mu = 0)) |>
  dplyr::select(estimate, p.value)
}

##Repeat the simulation 5,000 times for each μ
sim_results =
  expand_grid(mu = mu_values, iter = 1:5000) |>
  mutate(test_res = map(mu, sim_ttest)) |>
  unnest(test_res) |>
  mutate(reject = p.value < 0.05)
```

**3) Plot power curve and mean-estimate plots**
```{r P2_3}
## power curve
power_df =
  sim_results |>
  group_by(mu) |>
  summarize(power = mean(reject), .groups = "drop")

power_df |>
  ggplot(aes(x = mu, y = power)) +
  geom_line() +
  geom_point(size = 2, color = "skyblue") +
  theme_minimal() +
  labs(
    title = "Figure 2: Power for One-Sample t-Test (n = 30, σ = 5, α = 0.05)",
    x = "True mean (μ)",
    y = "Power of the test"
  ) 
```

**Explanation:** As the true mean increases, the power of the test also increases. Therefore, there is a positive association between effect size and power, meaning that larger effect sizes are more likely to lead the test to detect a true difference and reject the null hypothesis.
From the beginning, the true mean is close to 0 and the power is around 0.05, reflecting the significance level of 5%.

```{r P2_4}
## mean-estimate plots
mean_estimates =
  sim_results |>
  group_by(mu) |>
  summarize(
    mean_all   = mean(estimate),
    mean_reject = mean(estimate[reject]),
    .groups = "drop"
  )

mean_estimates |>
  pivot_longer(mean_all:mean_reject,
               names_to = "set", values_to = "avg_est") |>
  mutate(set = recode(set,
                      mean_all   = "All samples",
                      mean_reject = "Only when the null (H0) rejected")) |>
  ggplot(aes(x = mu, y = avg_est, color = set)) +
  geom_line() +
  geom_point(size = 2) +
  theme_minimal() +
  labs(
    title = "Figure 3: Average Estimated Mean vs True Mean",
    x = "True mean (μ)",
    y = "Average of sample mean"
  ) 
```

**Is the average of sample mean across tests for which the null is rejected approximately equal to the true value of μ? Why or why not?**

**Answer:** Even though the red line (all samples) and the blue line (only when rejecting the null) look similar at higher true means, their behavior is different when the true mean is small. The red line is close to the true mean across all values of μ because the sample mean is an unbiased estimator. However, the blue line is consistently higher than the red line when μ is small. This happens because we are only looking at samples where the null hypothesis is rejected, and those samples tend to have larger estimated means. As a result, the average of the sample means in the rejected group overestimates the true mean.

## Problem 3: The Washington Post - Homicides

**1) Import data**
```{r P3_import, warning = FALSE}
homicide_df = read.csv("./data/homicide-data.csv") |> 
  janitor::clean_names()
## Variables
names(homicide_df)
```

The `homicide-data.csv` is the dataset from the Washington Post which has gathered data on homicides in 50 large U.S. cities. There are `r ncol(homicide_df)` variables and `r format(nrow(homicide_df), big.mark = ",")` homicide cases in this raw dataset. Some of key variables are `city`,`state`,`victim_sex`, and `disposition` (case status).

**2) Create a `city_state` variable and summarize numbers of homicides and unsolved cases within cities**
```{r P3_summarizecases}
## Create a `city_state` variable (e.g. “Baltimore, MD”)
homicide_df = homicide_df |>
  mutate(
    city_state = str_c(city, ", ", state),
    unsolved = disposition %in% c("Closed without arrest", "Open/No arrest")
)

## Summarize numbers of homicides and unsolved cases within cities
homicide_summary = homicide_df |>
  group_by(city_state) |>
  summarize(
    total = n(),
    unsolved = sum(unsolved),
    solved = total - unsolved,
    .groups = "drop"
)

homicide_summary
```

After creating the new variable `city_state`, there are `r n_distinct(homicide_df$city_state)` different large cities in the dataset, including 50 large cities in different states and the federal district of Washington, DC.

**3) For Baltimore, MD, use the prop.test function to estimate the proportion of homicides that are unsolved, and estimate proportion and confidence intervals**

```{r P3_Baltimore, warning = FALSE}
baltimore = homicide_summary |>
  filter(city_state == "Baltimore, MD")

baltimore_test =
  prop.test(baltimore$unsolved, baltimore$total)

baltimore_tidy = broom::tidy(baltimore_test) |>
    select(estimate, conf.low, conf.high)

baltimore_tidy
```

**Answer:** The estimated proportion of homicides in Baltimore, MD that are unsolved is 64.6% (95% CI: 62.8% - 66.3%)


**4) Now run prop.test for each of the cities in your dataset, and extract both the proportion of unsolved homicides and the confidence interval for each**

```{r P3_unsolved, warning = FALSE}
city_results = homicide_summary |>
  mutate(
    prop_test = map2(unsolved, total, ~ prop.test(.x, .y)),
    tidy_res = map(prop_test, broom::tidy)
  ) |>
  unnest(tidy_res) |>
  select(city_state, total, unsolved, estimate, conf.low, conf.high)

city_results
```

**5) Create a plot that shows the estimates and CIs for each city, and organize cities according to the proportion of unsolved homicides**

```{r P3_plot, fig.height = 18, fig.width = 8, warning = FALSE}
city_results |>
  arrange(estimate) |>
  mutate(city_state = fct_inorder(city_state)) |>
  ggplot(aes(x = estimate, y = city_state)) +
  geom_point(color = "skyblue") +
  geom_errorbarh(aes(xmin = conf.low, xmax = conf.high), width = 0.2) +
  labs(
    title = "Figure 4: Proportion of Unsolved Homicides by City",
    x = "Estimated proportion unsolved",
    y = "City"
  ) +
  theme_minimal(base_size = 12) +
  theme(
    axis.text.y = element_text(size = 10),
    axis.text.x = element_text(size = 10),
    plot.title = element_text(size = 15, face = "bold")
  )
```

**Explanation:** The plot illustrates that the proportion of unsolved homicides varies a lot across cities. Cities like _Chicago, IL_, _New Orleans, LA_, and _Baltimore, MD_ have higher proportions of unsolved cases, while cities such as _Richmond, VA_ and _Charlotte, NC_ have much lower proportions. Cities with more homicide cases tend to have narrower confidence intervals because the estimates are more stable. In contrast, cities with only a few cases have wider confidence intervals due to greater uncertainty. For example, _Tulsa, AL_ has an estimated proportion of 0 unsolved cases, but its confidence interval is very wide because the number of total homicides is small.

